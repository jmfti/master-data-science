{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1  googleplaystoreV3.csv  texto.txt\r\n"
     ]
    }
   ],
   "source": [
    "#Comprobamos la existencia de los dataset\n",
    "!ls /dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creamos un directorio para almacenar los datos de entrada\n",
    "!hadoop fs -mkdir /user/root/input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Copiamos el fichero texto.txt a HDFS\n",
    "!hadoop fs -copyFromLocal /dataset/texto.txt /user/root/input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 items\r\n",
      "-rw-r--r--   3 root supergroup         92 2021-06-20 11:20 /user/root/input/texto.txt\r\n"
     ]
    }
   ],
   "source": [
    "#Comprobamos que se ha importado correctamente y vemos la información \n",
    "!hadoop fs -ls /user/root/input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Esto es un ejemplo de programación mapreduce.\r\n",
      "Esto es un ejemplo de contador de palabras. \r\n"
     ]
    }
   ],
   "source": [
    "#Mostramos el contenido de texto.txt\n",
    "!hadoop fs -cat /user/root/input/texto.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La programación MapReduce para Hadoop puede hacerse utilizando una gran variedad de lenguajes de programación. Por su utilidad en el análisis de datos y su sencillez y rápido aprendizaje utilizaremos Python. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/env python3\n",
    "import sys\n",
    "\n",
    "#Leemos cada linea de la entrada estandar\n",
    "for line in sys.stdin:  \n",
    "    #Dividimos en palabras cada linea\n",
    "    words = line.split()  \n",
    "    #Iteramos sobre cada palabra contenida en words\n",
    "    for word in words:    \n",
    "        #Escribimos por salida estandar el par <Clave,Valor>\n",
    "        #En este caso la Clave será la palabra en cuestión y el valor será 1\n",
    "        print(f\"{word}\\t1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Esto\t1\r\n",
      "es\t1\r\n",
      "un\t1\r\n",
      "ejemplo\t1\r\n",
      "de\t1\r\n",
      "programación\t1\r\n",
      "mapreduce.\t1\r\n",
      "Esto\t1\r\n",
      "es\t1\r\n",
      "un\t1\r\n",
      "ejemplo\t1\r\n",
      "de\t1\r\n",
      "contador\t1\r\n",
      "de\t1\r\n",
      "palabras.\t1\r\n"
     ]
    }
   ],
   "source": [
    "#Para que el script de Python se ejecute, necesita permisos de ejecución. Probamos a ejecutar en local.\n",
    "!chmod +x mapper.py\n",
    "!cat /dataset/texto.txt | ./mapper.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/env python3\n",
    "import sys\n",
    "\n",
    "curr_word = None\n",
    "curr_count = 0\n",
    "\n",
    "# Procesamos cada Clave-Valor que produce la funcion map\n",
    "for line in sys.stdin:\n",
    "\n",
    "    #Dividimos la clave y el valor\n",
    "    word, count = line.split('\\t')\n",
    "\n",
    "    #Convertimos a entero el valor\n",
    "    count = int(count)\n",
    "\n",
    "    #Si la palabra actual es la misma que la previa, incrementamos el contador\n",
    "    #en otro caso imprimimos la palabra en cuestion con el contador\n",
    "    if word == curr_word:\n",
    "        curr_count += count\n",
    "    else:\n",
    "        if curr_word:   \n",
    "            print (f\"{curr_word}\\t{curr_count}\")\n",
    "        \n",
    "        #Actualizamos las variables a la nueva palabra\n",
    "        curr_word = word\n",
    "        curr_count = count\n",
    "\n",
    "#Imprimimos la ultima palabra\n",
    "if curr_word == word:\n",
    "      print (f\"{curr_word}\\t{curr_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Esto\t2\r\n",
      "contador\t1\r\n",
      "de\t3\r\n",
      "ejemplo\t2\r\n",
      "es\t2\r\n",
      "mapreduce.\t1\r\n",
      "palabras.\t1\r\n",
      "programación\t1\r\n",
      "un\t2\r\n"
     ]
    }
   ],
   "source": [
    "#Otorgamos permisos de ejecución al script reducer y probamos el programa completo en local. \n",
    "#El comando sort de unix hace de fase sort&shuffle\n",
    "!chmod +x reducer.py\n",
    "!cat /dataset/texto.txt | ./mapper.py | sort | ./reducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-06-20 11:24:32,635 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
      "packageJobJar: [/media/notebooks/Leccion1.1/reducer.py, /media/notebooks/Leccion1.1/mapper.py] [/app/hadoop-3.3.1/share/hadoop/tools/lib/hadoop-streaming-3.3.1.jar] /tmp/streamjob8416506297260999455.jar tmpDir=null\n",
      "2021-06-20 11:24:33,335 INFO client.DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at yarnmaster/172.19.0.7:8032\n",
      "2021-06-20 11:24:33,466 INFO client.DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at yarnmaster/172.19.0.7:8032\n",
      "2021-06-20 11:24:33,647 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/root/.staging/job_1624180770116_0001\n",
      "2021-06-20 11:24:34,034 INFO mapred.FileInputFormat: Total input files to process : 1\n",
      "2021-06-20 11:24:34,105 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "2021-06-20 11:24:34,224 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1624180770116_0001\n",
      "2021-06-20 11:24:34,224 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
      "2021-06-20 11:24:34,371 INFO conf.Configuration: resource-types.xml not found\n",
      "2021-06-20 11:24:34,372 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\n",
      "2021-06-20 11:24:34,735 INFO impl.YarnClientImpl: Submitted application application_1624180770116_0001\n",
      "2021-06-20 11:24:34,766 INFO mapreduce.Job: The url to track the job: http://yarnmaster:8088/proxy/application_1624180770116_0001/\n",
      "2021-06-20 11:24:34,767 INFO mapreduce.Job: Running job: job_1624180770116_0001\n",
      "2021-06-20 11:24:40,842 INFO mapreduce.Job: Job job_1624180770116_0001 running in uber mode : false\n",
      "2021-06-20 11:24:40,844 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "2021-06-20 11:24:45,917 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "2021-06-20 11:24:48,937 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "2021-06-20 11:24:48,958 INFO mapreduce.Job: Job job_1624180770116_0001 completed successfully\n",
      "2021-06-20 11:24:49,054 INFO mapreduce.Job: Counters: 54\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=157\n",
      "\t\tFILE: Number of bytes written=832108\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=334\n",
      "\t\tHDFS: Number of bytes written=84\n",
      "\t\tHDFS: Number of read operations=11\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=5301\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=1711\n",
      "\t\tTotal time spent by all map tasks (ms)=5301\n",
      "\t\tTotal time spent by all reduce tasks (ms)=1711\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=5301\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=1711\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=5428224\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=1752064\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=2\n",
      "\t\tMap output records=15\n",
      "\t\tMap output bytes=121\n",
      "\t\tMap output materialized bytes=163\n",
      "\t\tInput split bytes=196\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=9\n",
      "\t\tReduce shuffle bytes=163\n",
      "\t\tReduce input records=15\n",
      "\t\tReduce output records=9\n",
      "\t\tSpilled Records=30\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=143\n",
      "\t\tCPU time spent (ms)=1420\n",
      "\t\tPhysical memory (bytes) snapshot=804147200\n",
      "\t\tVirtual memory (bytes) snapshot=7684096000\n",
      "\t\tTotal committed heap usage (bytes)=836763648\n",
      "\t\tPeak Map Physical memory (bytes)=304427008\n",
      "\t\tPeak Map Virtual memory (bytes)=2559344640\n",
      "\t\tPeak Reduce Physical memory (bytes)=198975488\n",
      "\t\tPeak Reduce Virtual memory (bytes)=2566344704\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=138\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=84\n",
      "2021-06-20 11:24:49,054 INFO streaming.StreamJob: Output directory: /user/root/output\n"
     ]
    }
   ],
   "source": [
    "#Ejecutamos el programa MapReduce en el clúster Hadoop. \n",
    "!mapred streaming \\\n",
    "  -input /user/root/input/texto.txt \\\n",
    "  -output /user/root/output \\\n",
    "  -file /media/notebooks/Leccion1.1/reducer.py \\\n",
    "  -file /media/notebooks/Leccion1.1/mapper.py \\\n",
    "  -mapper mapper.py \\\n",
    "  -reducer reducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 items\r\n",
      "-rw-r--r--   3 root supergroup          0 2021-06-20 11:24 /user/root/output/_SUCCESS\r\n",
      "-rw-r--r--   3 root supergroup         84 2021-06-20 11:24 /user/root/output/part-00000\r\n"
     ]
    }
   ],
   "source": [
    "#El resultado se almacena en el directorio especificado en el comando.\n",
    "#Se crea un archivo _SUCCESS que indica la correcta ejecución del programa.\n",
    "#El resultado se almacena en el fichero part-00000\n",
    "!hadoop fs -ls /user/root/output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Esto\t2\r\n",
      "contador\t1\r\n",
      "de\t3\r\n",
      "ejemplo\t2\r\n",
      "es\t2\r\n",
      "mapreduce.\t1\r\n",
      "palabras.\t1\r\n",
      "programación\t1\r\n",
      "un\t2\r\n"
     ]
    }
   ],
   "source": [
    "#Comprobamos el resultado \n",
    "!hadoop fs -cat /user/root/output/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
