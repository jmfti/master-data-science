---
title: "Work"
author: "jmfti"
date: "2022-09-10"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(MASS)
library(e1071)
library(ISLR2)
library(ggplot2)
library(class)
library(glmnet)
library(caret)
library(tidyverse)
library(dplyr)
```
# Cap1 - Linear regression
Methods to fit a linear function over some parameters (w) against a test data 
$$y = \sum_{i=1}^n{w_{i} x_{i}} + w_0$$
Given vector $t$ of observations, we need to find a function $y(x, w) = w_0 + w_1 x_1 + ... + w_n x_n$
$$
\begin{align} y(\textbf{x},\textbf{w}) = \sum _{j=0} ^{M} \left ( w_j\phi_j(\textbf{x}) \right ) = 
\textbf{w}^T \Phi
\end{align}
$$
So that $\textbf{w} = (w_0, ..., w_M-1)^T$ y $\Phi = (\phi_0, ..., \phi_D)^T$.
$\Phi(\textbf{x})$ is a matrix, with as many columns as base functions has  $\phi(x_i)$ 

Assuming \textbf{t} = y(\textbf{x,w}) + \epsilon so that \epsilon \sim N(0, \lambda^{-1}) beeing \lambda the precision, we can write
$$
\begin{align}
p(\textbf{t} \mid \textbf{x}, \textbf{w}, \Lambda) = N(\textbf{t} \mid y(\textbf{x},\textbf{w}), \lambda^{-1}) 
\end{align}
$$
the probability of objective value $t$ given inputs $\textbf{x}$ and a coefficients vector $\textbf{w}$ is the probability of having this observation distributed in a Normal $y(\textbf{x},\textbf{w})$ with inverse precision $\lambda$

Assuming a cuadratic error function, the optimal prediction for a new value of x will come determined by the conditional mean 
$$
\begin{align} 
E[t\mid \textbf{x}] = \int{ t p(t\mid \textbf{x}) dt = y(\textbf{x},\textbf{w}) } 
\end{align}
$$
So assuming a target vector $\textbf{t} = (t_1, ..., t_n)$ and a set of variables $\textbf{X}=\{x_1, x_2,...,x_n\}$, Assuming independence of all events, the likelihood function is (according to Bishop) the product of all the observations 
$$
\begin{align} p(\textbf{t} \mid \textbf{X}, \textbf{w}, \lambda) = p(t_1 \mid \textbf{x}_1, \textbf{w}, \lambda) p(t_2 \mid \textbf{x}_2, \textbf{w}, \lambda) ... p(t_n \mid \textbf{x}_n, w, \lambda) 
\end{align}
$$
$$
\begin{align} p(\textbf{t} \mid \textbf{X}, \textbf{w}, \lambda ) = \prod _{n=1} ^N N(t_n \mid \textbf{w}^T\phi(\textbf{x}_n), \lambda^{-1} ) 
\end{align}
$$
Taking the log likelihood product turns into a sum
$$ 
\begin{align}
\ln p(\textbf{t} \mid \textbf{X}, \textbf{w}, \lambda) &= \sum _{n=1} ^N \ln N(t_n \mid w^T, \phi(x_n), \lambda^{-1}) \\ 
&= \dfrac{N}{2} \ln \lambda - \dfrac{N}{2} \ln (2\pi) - \lambda \left ( \dfrac{1}{2} \sum _{n=1} ^N \left [ t_n - w^T \phi(x_n) \right ]^2 \right )  
\end{align}
$$
Now to find the best matching values for \textbf{w} that makes more likely the vector t we find the gradient of the function and set to 0.
We derive the function in respect to probability, so when we set to 0, we are trying find a w vector that maximizes the probability
$$
\begin{align}
\nabla \ln p(\textbf{t} \mid \textbf{X}, \textbf{w}, \lambda) = \sum _{n=1} ^N \left ( t_n - \textbf{w}^T \phi(x_n) \right )\phi(x_n)^T 
\end{align}
$$

maximize setting the gradient
$$
\begin{align}
0= \sum _{n=1} ^N t_n \phi(x_n)^T - \sum _{n=1} ^N \textbf{w}^T\phi(x_n)\phi(x_n)^T
\end{align}
$$
$w^T$ is just a constant so
$$
\begin{align}
0= \sum _{n=1} ^N t_n \phi(x_n)^T - \textbf{w}^T\sum _{n=1} ^N \phi(x_n)\phi(x_n)^T
\end{align}
$$
If $\sum _{n=1} ^N \phi(x_n)\phi(x_n)^T = \Phi^T\Phi$ beeing $\Phi$ the design matrix $\Phi_{nj} = \phi_j(x_n)$ and  
$
\sum_{n=1}^N t_n \phi(x_n)^T = \textbf{t}^T \Phi
$
then
$$
\begin{align} 
0 &= \textbf{t}^T  \Phi - \textbf{w}^T  \Phi^T  \Phi \\
\textbf{w}^T &= \textbf{t}^T  \Phi ( \Phi^T  \Phi)^{-1} 
\end{align}
$$

so 

$$
\begin{align}
\textbf{w} &= ( \Phi^T  \Phi)^{-1}  \Phi^T \textbf{t}
\end{align}
$$
That gives us the parameters for the model

Para la precisión también podemos hacer el mismo procedimiento derivando respecto a $\lambda$, quedándonos
$$
\begin{align}
\dfrac{1}{\lambda} = \dfrac{1}{N} \sum _{i=1} ^N \left ( t_i - w^T \phi(x_i) \right )^2
\end{align}
$$
## Estimación de coeficientes por máxima verosimilitud
Asumiendo que $t = y(x,w) + \epsilon$ tal 

```{r}
#library(MASS)
#library(ISLR2)
head(Boston)
```

## Including Plots

You can also embed plots, for example:

```{r pressure, echo=FALSE}
plot(pressure)
```

Note that the `echo = FALSE` parameter was added to the code chunk to prevent printing of the R code that generated the plot.

# Simple Linear regression

```{r}
lm.fit <- lm(medv ~ lstat, data=Boston)
lm.fit
summary(lm.fit)

names(lm.fit)

lm.fit$coefficients
head(lm.fit$residuals)
head(lm.fit$effects)
# get an estimated interval of each coefficient
confint(lm.fit)
```

Confidence intervals for prediction and mean can be obtained.

## Intervalo de confianza para la media de y
```{r}
predict(lm.fit, data.frame(lstat=(c(5,10,15))), interval="confidence")
```

## Intervalo de predicción para y
```{r}
predict(lm.fit, data.frame(lstat=(c(5,10,15))), interval="prediction")
```

```{r}
plot(Boston$lstat, Boston$medv)
abline(lm.fit)
abline(lm.fit, lwd=3, col="red")
```

## Diagnostic plots
```{r}
par(mfrow=c(2,2))
plot(lm.fit)
```

```{r}
plot(predict(lm.fit), residuals(lm.fit))
```

### Studentized residuals
```{r}
plot(predict(lm.fit), rstudent(lm.fit))
```

```{r}
plot(hatvalues(lm.fit))
which.max(hatvalues(lm.fit))
```

Let's say we have 2 populations of n = 100 elements. A population has a parameter that has a distribution over the population of N(90,2).
If we apply X1 quantity of treatment A, effect?
If we apply x2 quantity of treatment A, effect?


```{r}
gen_pop = rnorm(100, 90, 2)
df_gen_pop = data.frame("measure_a" = gen_pop, "applied_units" = rep(0,100))
# now let's say treatment adds 5 + error for each unit, where error is standard N(0,1)
treated_1_unit = rnorm(100, 90, 2) + 1 * 5 + rnorm(100, 0, 1)
df_treat1 = data.frame("measure_a" = treated_1_unit, "applied_units" = rep(1,100))

treated_2_unit = rnorm(100, 90, 2) + 2 * 5 + rnorm(100, 0,1)
df_treat2 = data.frame("measure_a" = treated_2_unit, "applied_units" = rep(2,100))

final_df = rbind(df_gen_pop, df_treat1)
final_df = rbind(final_df, df_treat2)

#library(ggplot2)

ggplot(final_df) +
  aes(
    x = measure_a,
    fill = applied_units,
    colour = applied_units,
    group = applied_units
  ) +
  geom_density(adjust = 1L) +
  scale_fill_distiller(palette = "Accent", direction = 1) +
  scale_color_distiller(palette = "Accent", direction = 1) +
  theme_classic()

ggplot(final_df) +
  aes(
    x = "",
    y = measure_a,
    fill = applied_units,
    colour = applied_units,
    group = applied_units
  ) +
  geom_violin(adjust = 1L, scale = "area") +
  scale_fill_distiller(palette = "Accent", direction = 1) +
  scale_color_distiller(palette = "Accent", direction = 1) +
  theme_classic()

model = lm(measure_a ~ applied_units, data=final_df)
summary(model)

# coefficients
confint(model)

```
Confidence interval for the mean, without applying any measure is 90.56 ~ 90.31
Confidence interval for the mean of slope applied_units: 4.72 ~ 5.31, so this suggests that treatment adds N(5,\beta)

## Multiple linear regression

```{r}
lm.fit <- lm(medv ~ lstat + age, data=Boston)

summary(lm.fit)

lm.fit <- update(lm.fit, ~ . - age)

summary(lm.fit)
```

### More variables

Sometimes it's easier to get a full model and just prune it with the Akaike criteria (AIC)

```{r, echo=FALSE}
# add all terms and add all combined effects 2 on 2 (medv * age + medv*nox + ...)
lm.fit <- lm(medv ~ . + .*., data=Boston)

pruned_model <- step(lm.fit, direction="backward")
summary(pruned_model)
```

```{r}
summary(pruned_model)
```

### Anova test

anova performs a hypothesis test comparing the 2 models. Null hypothesis is that the two models fit the data equally well, the alternative hypothesis is that the full model is superior.

```{r}
first_model <- lm(medv ~ lstat + age, data=Boston)
anova(first_model, lm.fit)
```

A much better model having p close to 0 and F quite low, also the residual sum of squares (RSS) is much more lower in model 2 than in model 1. But it comes at a cost. Those 76 degrees of freedom.

### Cualitative predictors

```{r}
head(Carseats)
carseats_model_1 <- lm(Sales ~ . + Income:Advertising + Price:Age, data=Carseats)
```

# Classification methods

## Two classes 
A discriminant function is a function that takes a vector of inputs x and tells us if it belongs to Class $C_1$ or $C_2$.
The form of the function is $y(x) = w^T x + w_0$, where $w^T$ is the weights vector and x is the features matrix.

The decision boundary is defined by the relation $y(x) = 0$ which corresponds to a $D-1$ dimensional hyperplan within a D-dimensional input space.

## Stock Market Data

```{r}
names(Smarket)
dim(Smarket)
summary(Smarket)
pairs(Smarket)
plot(Smarket$Volume)
```

## Logistic regression

Logistic regression is equivalent to linear regression in the fact that we also model a prediction as a 
function of a sum of terms.

For this it is used the logistic function
$$ p(x) = \dfrac{e^{\beta_0 + \beta_1 x}} {{1 + e^{\beta_0 + \beta_1 x}}}  $$
Manipulating a bit 
$$  \dfrac{p(x)}{1-p(x)} = e^{\beta_0 + \beta_1 x} $$
The left hand is called ODDS, takes values between 0 and $\inf$. If p(x) = 0.8, then the odds is 0.8/0.2.
"On average, nine out of ten people (p(x) = 0.9 => 0.9/0.1 = odds) with an odds of 9 will default"
Making the log of both sides
$$  log \left ( \dfrac{p(x)}{1-p(x)} \right ) = \beta_0 + \beta_1 x $$
So, coefficients here play the following role. An increase in one unit in $\beta_1$ increases the log odds by $value(\beta_1)$

We will fit a logistic regression to predict with Lag1 ... Lag5
Use glm() with arg family = binomial
```{r}
glm.fits <- glm(Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + Volume, data=Smarket, family=binomial)
summary(glm.fits)
coef(glm.fits)
```
Coefficient for Lag 1 is -0.073. This means that 1 unit increase in Lag1 is associated with an increase in the log odds of Direction by -0.073 units.
```{r}
# access some aspects of summary
summary(glm.fits)$coef
summary(glm.fits)$coef[,4]
```

Make judgements with data

```{r}
glm.probs <- predict(glm.fits, type="response")
glm.probs[1:10]
contrasts(Smarket$Direction)
# R will try to create a dummy variable
glm.pred <- rep("Down", 1250) # create a new variable with Down
glm.pred[glm.probs > .5] = "Up" # comparing both vectors, when glm.probs[i] > .5, glm.pred[i] = Up
table(glm.pred, Smarket$Direction) # create a confusion matrix. Diagonal = correct, Off-diagonal = incorrect
# when predicted Down, 145 were correctly Down, but 141 predicted Down and should be Up

# we can get the precision of our prediction
mean(glm.pred == Smarket$Direction)

# error rate is 
1 - mean(glm.pred == Smarket$Direction)

# prepare a train data set out of the big one
train <- Smarket$Year < 2005
Smarket.2005 <- Smarket[!train, ]
dim(Smarket.2005)
Smarket.Direction.2005 <- Smarket$Direction[!train]

glm.fits <- glm( Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + Volume, 
                 data = Smarket, family = binomial, subset = train)
summary(glm.fits)
coef(glm.fits)
glm.probs <- predict(glm.fits, Smarket.2005, type="response")
glm.pred <- rep("Down", 252)
glm.pred[glm.probs > .5] <- "Up"
table(glm.pred, Smarket.Direction.2005)

mean(glm.pred == Smarket.Direction.2005) # accuracy compared with Direction 2005 

mean(glm.pred != Smarket.Direction.2005) # accuracy in others
```
## Linear Discriminant Analysis

```{r}
lda.fit <- lda(Direction ~ Lag1 + Lag2, data=Smarket, subset=train)
lda.fit
summary(lda.fit)
plot(lda.fit)

```

```{r}
# creamos 2 grupos con diferente distribucion de 2 variables (x,y)
grupo1_x = rnorm(100, mean=-2, sd=2)
grupo1_y = rnorm(100, mean=0, sd=0.5)
grupo2_x = rnorm(100, mean=3, sd=1)
grupo2_y = rnorm(100, mean=2, sd=2)
ndf = data.frame(x = c(grupo1_x, grupo2_x), y = c(grupo1_y, grupo2_y), group = c(rep(1, 100), rep(2, 100)))
ndf$group = factor(ndf$group)

bin.kde = kde2d(ndf$x, ndf$y, n=50)
image(bin.kde)
contour(bin.kde, add=T)

lda.fit2 = lda(group ~ x + y, data=ndf)
plot(lda.fit2)
lda.pred = predict(lda.fit2)
lda.class = lda.pred$class 
table(lda.class, ndf$group)
mean(lda.class == ndf$group) # precision

# ahora usamos un subset
train = sample(c(T,F), 200, replace=T, prob=c(0.5,0.5))
lda.fit3 = lda(group ~ x + y, data=ndf, subset=train)
plot(lda.fit3)
lda.pred3 = predict(lda.fit3, ndf[!train,])
lda.class3 = lda.pred3$class
table(lda.class3, ndf$group[!train])
mean(lda.class3 == ndf$group[!train])

```

### 10-fold CV LDA

```{r}
#library(caret)

fitControl <- trainControl(method = "cv", number = 10)

fit.lda.cv <- train(group ~ x + y,
             method     = "lda",
             #tuneGrid   = expand.grid(k = 1:10),
             preProcess = c("center", "scale"),
             trControl  = fitControl,
             metric     = "Accuracy",
             data       = ndf)
fit.lda.cv
#plot(fit.lda.cv)
```

## Quadratic Discriminant analysis

```{r}
qda.fit <- qda(Direction ~ Lag1 + Lag2, data=Smarket, subset=train)
qda.fit

summary(qda.fit)

# get the predictions 
qda.class <- predict(qda.fit, Smarket.2005)$class
table(qda.class, Smarket.Direction.2005)
mean(qda.class == Smarket.Direction.2005)

```

## Naive Bayes

```{r}
#library(e1071)
nb.fit <- naiveBayes(Direction ~ Lag1 + Lag2, data=Smarket, subset=train)
nb.fit

summary(nb.fit)

head(predict(glm.fits, Smarket.2005, type="response"))
head(predict(nb.fit, Smarket.2005, type="raw"))


```

## KNN (K nearest neighbors)

knn(<matrix_x>, <new_x>, <groups for matrix_x>, k)
It returns a prediction for <new_x>

```{r}
#library(class)
train.x <- cbind(Smarket$Lag1, Smarket$Lag2)[train,]
test.x <- cbind(Smarket$Lag1, Smarket$Lag2)[!train,]
train.direction <- Smarket$Direction[train]
knn.pred <- knn(train.x, test.x, train.direction, k=1) # knn.pred has predictions for test.x
table(knn.pred, Smarket$Direction[!train])
mean(knn.pred == Smarket$Direction[!train]) # check proportion of test.x predictions correct (!train)

# we try setting the hyperparameter k to 3
knn.pred <- knn(train.x, test.x, train.direction, k=3)
table(knn.pred, Smarket$Direction[!train])
mean(knn.pred == Smarket$Direction[!train])
```

Now let's make a more random data set

```{r}
train <- sample(c(TRUE, FALSE), nrow(Smarket), replace=TRUE, prob=c(0.7, 0.3)) # 70% for train, 30% for test
train.x <- cbind(Smarket$Lag1, Smarket$Lag2)[train,]
test.x <- cbind(Smarket$Lag1, Smarket$Lag2)[!train,]
train.direction <- Smarket$Direction[train]
knn.pred <- knn(train.x, test.x, train.direction, k=1)
table(knn.pred, Smarket$Direction[!train])
mean(knn.pred == Smarket$Direction[!train])

# we try setting the hyperparameter k to 3
knn.pred <- knn(train.x, test.x, train.direction, k=3)
table(knn.pred, Smarket$Direction[!train])
mean(knn.pred == Smarket$Direction[!train])

```

When we have variables with high differences in values it can affect the prediction results.
A difference of 1000 dolars could be much more signifficant than a difference of 50 years, but speaking in absolute terms the salary will have more impact on predictions due to its scale.

So to prevent this effects we standardize variables

```{r}
set.seed(1)
standardized.x <- scale(Caravan[,-86]) # get our data set standardized without the 86th column (Purchase)
var(Caravan[,1])
test <- 1:1000 # divide dataset. We we'll get 1000 values for testing, the rest will be used for training
# well, it's just wrong. better
train <- sample(c(TRUE, FALSE), nrow(Caravan), replace=TRUE, prob=c(0.7, 0.3)) # 70% for train, 30% for test
train.x <- standardized.x[train,]
test.x <- standardized.x[!train,]
train.y <- Caravan$Purchase[train] # train y is Purchase 
test.y <- Caravan$Purchase[!train] # in y we put Purchase

knn.pred <- knn(train.x, test.x, train.y, k=1)
mean(test.y == knn.pred) # accuracy of predictions against test datat
# success rate
table(knn.pred, test.y)

knn.pred <- knn(train.x, test.x, train.y, k=3)
mean(test.y == knn.pred) # accuracy of predictions against test data
table(knn.pred, test.y)

```

## Cross Validation KNN
Now use the new dataset created with bivariate data. Use 10 CV-fold to select the hyperparameter K.

```{r}
library(caret)

fitControl <- trainControl(## 10-fold CV
                           method = "repeatedcv",
                           number = 10,
                           ## repeated ten times
                           repeats = 10)

fit.knn <- train(group ~ x + y,
             method     = "knn",
             tuneGrid   = expand.grid(k = 1:10),
             trControl  = fitControl,
             metric     = "Accuracy",
             data       = ndf)
fit.knn
plot(fit.knn)
```



# Ridge regression

```{r}
#library(glmnet)
#library(ISLR2)
head(Hitters)
Hitters = na.omit(Hitters)
grid = 10^seq(10, -2, length=100)
# model.matrix adds the intercept in a usual regression model
x = model.matrix(Salary ~ ., Hitters)[, -1] # convert categorical, remove the Intercept ([, -1])
nrow(x)
y = Hitters$Salary
nrow(y)
# glmnet performs ridge/lasso (depending on alpha, alpha=1 lasso, alpha=0 ridge) on a grid of lambdas and returns 
# the best lambda
# glmnet also standardizes input variables, so they are in the same scale
ridge.mod = glmnet(x, y, alpha=0, lambda=10^seq(10, -2, length=100))
dim(coef(ridge.mod))
ridge.mod$lambda[50] # lambda = 11498
ridge.mod$lambda[60] # lambda = 705
# for this lambda we have the next coefficients
coef(ridge.mod)[, 50] # AtBat 0.037
coef(ridge.mod)[, 60] # AtBat 0.112, lambda decreased, so coefficient increased

# As lambda increases, 
```

With train / test

```{r}
train = rbinom(nrow(Hitters), 1, 0.5) > 0.5
test_dataset = Hitters[!train,]
train_dataset = Hitters[train,]
y.test = y[!train]
y.train = y[train]

ridge.mod = glmnet(x[train,], y[train], alpha=0, lambda=grid, thresh=1e-12); 
# predict using lambda = 4
ridge.pred = predict(ridge.mod, s=4, newx=x[!train,]); 
mean((ridge.pred - y.test)^2)

# predict using lambda = 10
ridge.pred = predict(ridge.mod, s=10, newx=x[!train,])

# setting lambda = 0 is least squares
ridge.pred = predict(ridge.mod, s=0, newx=x[!train,])

# set lambda using CV
cv.out = cv.glmnet(x[train,], y[train], alpha=0)
plot(cv.out)
best_lambda = cv.out$lambda.min
best_lambda

ridge.pred = predict(ridge.mod, s=best_lambda, newx=x[!train,])
mean((ridge.pred - y.test)^2)
```

## Lasso

```{r}
lasso.mod = glmnet(x[train,], y[train], alpha=1, lambda=grid)
plot(lasso.mod)

cv.out = cv.glmnet(x[train,], y[train], alpha=1)
plot(cv.out)
best_lambda = cv.out$lambda.min
lasso.pred = predict(lasso.mod, s = best_lambda, newx = x[!train,])
mean((lasso.pred - y.test)^2)

lasso.coef = predict(lasso.mod, type="coefficients", s=best_lambda)
lasso.coef
```